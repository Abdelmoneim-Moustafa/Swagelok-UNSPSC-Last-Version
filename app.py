"""
üîç Swagelok UNSPSC Intelligence Platform - PRODUCTION VERSION
Professional data extraction tool for procurement teams

Features:
- Handles ALL part formats (2507-600-1-4, SS-4-TA, CWS-C.040-.405-P)
- Auto-save progress every 50 URLs
- Latest UNSPSC version detection
- Zero duplicates guarantee
- Professional user interface

Created by: Abdelmoneim Moustafa
Data Intelligence Engineer ‚Ä¢ Procurement Systems Expert
"""

import re
import time
import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, Optional, Set

# ==================== CONFIG ====================
MAX_WORKERS = 6  # Optimized for stability
COMPANY_NAME = "Swagelok"
CHECKPOINT_INTERVAL = 50
TIMEOUT = 20

# ==================== PAGE CONFIG ====================
st.set_page_config(
    page_title="Swagelok UNSPSC Intelligence Platform",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ==================== ENHANCED CSS ====================
st.markdown("""
<style>
    /* Main container */
    .main {
        background: #f8f9fa;
    }
    
    /* Header */
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 3rem 2rem;
        border-radius: 20px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 40px rgba(102, 126, 234, 0.3);
    }
    
    .main-header h1 {
        font-size: 2.8rem;
        font-weight: 800;
        margin-bottom: 0.5rem;
        text-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    
    .main-header h3 {
        font-size: 1.3rem;
        font-weight: 400;
        opacity: 0.95;
        margin-top: 0.5rem;
    }
    
    /* Feature boxes */
    .feature-box {
        background: white;
        padding: 1.5rem;
        border-radius: 15px;
        border-left: 5px solid #667eea;
        margin: 1rem 0;
        box-shadow: 0 4px 15px rgba(0,0,0,0.08);
    }
    
    .feature-box h4 {
        color: #667eea;
        font-weight: 700;
        margin-bottom: 0.5rem;
    }
    
    /* Success box */
    .success-box {
        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        padding: 2rem;
        border-radius: 20px;
        color: white;
        text-align: center;
        margin: 2rem 0;
        box-shadow: 0 8px 30px rgba(17, 153, 142, 0.3);
    }
    
    .success-box h2 {
        font-size: 2rem;
        font-weight: 700;
        margin-bottom: 1rem;
    }
    
    /* Checkpoint box */
    .checkpoint-box {
        background: #fff3e0;
        border-left: 5px solid #ff9800;
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 4px 15px rgba(255, 152, 0, 0.1);
    }
    
    /* Info box */
    .info-box {
        background: #e3f2fd;
        border-left: 5px solid #2196f3;
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
    }
    
    /* Stats card */
    .stat-card {
        background: white;
        padding: 1.5rem;
        border-radius: 12px;
        text-align: center;
        box-shadow: 0 4px 12px rgba(0,0,0,0.06);
        border-top: 3px solid #667eea;
    }
    
    /* Button enhancement */
    .stButton>button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        font-weight: 600;
        border: none;
        border-radius: 12px;
        padding: 0.75rem 2rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
        transition: all 0.3s ease;
    }
    
    .stButton>button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(102, 126, 234, 0.4);
    }
    
    /* Download button */
    .stDownloadButton>button {
        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        color: white;
        font-weight: 600;
        border-radius: 12px;
        padding: 0.75rem 2rem;
        box-shadow: 0 4px 15px rgba(17, 153, 142, 0.3);
    }
    
    /* Progress tracking */
    .progress-text {
        font-size: 1.2rem;
        font-weight: 600;
        color: #667eea;
        text-align: center;
        padding: 1rem;
    }
    
    /* Footer */
    .footer {
        text-align: center;
        padding: 2rem;
        margin-top: 3rem;
        border-top: 2px solid #e0e0e0;
        color: #666;
    }
    
    .footer-name {
        color: #667eea;
        font-weight: 700;
        font-size: 1.1rem;
    }
</style>
""", unsafe_allow_html=True)

# ==================== PRODUCTION EXTRACTOR ====================
class ProductionExtractor:
    """Production-grade extractor with all fixes applied"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })
        self.processed_urls: Set[str] = set()
        self.extracted_parts: Set[str] = set()
        
    def extract(self, url: str) -> Optional[Dict]:
        """Extract with comprehensive validation"""
        
        url = str(url).strip()
        
        if url in self.processed_urls:
            return None
        
        result = {
            "Part": "Not Found",
            "Company": COMPANY_NAME,
            "URL": url,
            "UNSPSC Feature (Latest)": "Not Found",
            "UNSPSC Code": "Not Found"
        }
        
        if not self._is_valid_url(url):
            self.processed_urls.add(url)
            return result
        
        try:
            response = self.session.get(url, timeout=TIMEOUT)
            
            if response.status_code != 200:
                self.processed_urls.add(url)
                return result
            
            soup = BeautifulSoup(response.text, "html.parser")
            html_text = response.text
            
            # Extract part number (FIXED for all formats)
            part_from_page = self._extract_part_from_page(soup, html_text)
            part_from_url = self._extract_part_from_url(url)
            
            # Validation logic
            if part_from_page and part_from_url:
                if self._normalize(part_from_page) == self._normalize(part_from_url):
                    if part_from_page not in self.extracted_parts:
                        result["Part"] = part_from_page
                        self.extracted_parts.add(part_from_page)
                else:
                    if part_from_url not in self.extracted_parts:
                        result["Part"] = part_from_url
                        self.extracted_parts.add(part_from_url)
            elif part_from_page and part_from_page not in self.extracted_parts:
                result["Part"] = part_from_page
                self.extracted_parts.add(part_from_page)
            elif part_from_url and part_from_url not in self.extracted_parts:
                result["Part"] = part_from_url
                self.extracted_parts.add(part_from_url)
            
            # Extract UNSPSC
            feature, code = self._extract_latest_unspsc(soup, html_text)
            if feature and code:
                result["UNSPSC Feature (Latest)"] = feature
                result["UNSPSC Code"] = code
            
            self.processed_urls.add(url)
            return result
            
        except requests.Timeout:
            self.processed_urls.add(url)
            return result
        except Exception:
            self.processed_urls.add(url)
            return result
    
    def _is_valid_url(self, url: str) -> bool:
        return isinstance(url, str) and url.startswith("http") and "swagelok.com" in url.lower()
    
    def _extract_part_from_page(self, soup, html_text) -> Optional[str]:
        """
        FIXED: Now handles ALL part formats:
        - 2507-600-1-4 (numbers first)
        - SS-4-TA (letters first)
        - CWS-C.040-.405-P (with dots)
        """
        patterns = [
            r'Part\s*#\s*:\s*([0-9A-Z][0-9A-Z.\-_/]*)',  # FIXED: [0-9A-Z] allows numbers first!
            r'Part\s+Number\s*:\s*([0-9A-Z][0-9A-Z.\-_/]*)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_text, re.IGNORECASE)
            for match in matches:
                if self._is_valid_part(match):
                    return match.strip()
        return None
    
    def _extract_part_from_url(self, url) -> Optional[str]:
        """Extract part from URL parameter"""
        patterns = [
            r'[?&]part=([0-9A-Z.\-_/%]+)',
            r'/p/([0-9A-Z.\-_/%]+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                part = match.group(1).replace('%2F', '/').replace('%252F', '/').strip()
                if self._is_valid_part(part):
                    return part
        return None
    
    def _normalize(self, part: str) -> str:
        if not part:
            return ""
        return re.sub(r'[.\-/\s]', '', part).lower()
    
    def _is_valid_part(self, part: str) -> bool:
        """
        FIXED: Now accepts number-only parts if length > 3
        Examples: "2507-600-1-4" ‚úÖ
        """
        if not isinstance(part, str) or not (2 <= len(part) <= 100):
            return False
        
        has_letter = any(c.isalpha() for c in part)
        has_number = any(c.isdigit() for c in part)
        
        # FIXED: Accept if has both, OR only numbers with length > 3
        if not (has_letter and has_number) and not (has_number and len(part) > 3):
            return False
        
        exclude = ['charset', 'utf', 'html', 'text', 'http']
        return not any(ex in part.lower() for ex in exclude)
    
    def _extract_latest_unspsc(self, soup, html_text) -> Tuple[Optional[str], Optional[str]]:
        """Extract LATEST UNSPSC version from specifications table"""
        versions = []
        
        for row in soup.find_all('tr'):
            cells = row.find_all('td')
            if len(cells) >= 2:
                attribute = cells[0].get_text(strip=True)
                value = cells[1].get_text(strip=True)
                vm = re.search(r'UNSPSC\s*\(([\d.]+)\)', attribute, re.IGNORECASE)
                if vm and re.match(r'^\d{6,8}$', value):
                    versions.append({
                        'version': self._parse_version(vm.group(1)),
                        'feature': attribute,
                        'code': value
                    })
        
        if not versions:
            pattern = r'UNSPSC\s*\(([\d.]+)\)[^\d]*?(\d{6,8})'
            matches = re.findall(pattern, html_text, re.IGNORECASE)
            for version_str, code in matches:
                versions.append({
                    'version': self._parse_version(version_str),
                    'feature': f"UNSPSC ({version_str})",
                    'code': code
                })
        
        if not versions:
            return None, None
        
        versions.sort(key=lambda x: x['version'], reverse=True)
        return versions[0]['feature'], versions[0]['code']
    
    def _parse_version(self, v: str) -> Tuple[int, ...]:
        try:
            return tuple(int(p) for p in v.split('.'))
        except:
            return (0,)

# ==================== STREAMLIT UI ====================

# Header
st.markdown("""
<div class="main-header">
    <h1>üîç Swagelok UNSPSC Intelligence Platform</h1>
    <h3>Professional Data Extraction ‚Ä¢ Enterprise Grade ‚Ä¢ Production Ready</h3>
</div>
""", unsafe_allow_html=True)

# Features section
st.markdown("""
<div class="feature-box">
    <h4>‚ú® Platform Features</h4>
    ‚Ä¢ <strong>Universal Part Support:</strong> Handles all formats (2507-600-1-4, SS-4-TA, CWS-C.040-.405-P)<br>
    ‚Ä¢ <strong>Smart Extraction:</strong> Latest UNSPSC version from specifications table<br>
    ‚Ä¢ <strong>Auto-Save:</strong> Progress saved every 50 URLs - never lose work<br>
    ‚Ä¢ <strong>Zero Duplicates:</strong> Guaranteed unique results<br>
    ‚Ä¢ <strong>Fast & Stable:</strong> Optimized for reliability and speed
</div>
""", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.markdown("### üìä Platform Settings")
    st.markdown(f"""
    <div class="info-box">
        <strong>Current Configuration:</strong><br>
        ‚Ä¢ Workers: {MAX_WORKERS}<br>
        ‚Ä¢ Timeout: {TIMEOUT}s<br>
        ‚Ä¢ Checkpoint: Every {CHECKPOINT_INTERVAL} URLs<br>
        ‚Ä¢ Status: Production Ready ‚úÖ
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("### üí° Best Practices")
    st.markdown("""
    <div class="feature-box">
        <strong>For Best Results:</strong><br>
        ‚úÖ Process 100-500 URLs per batch<br>
        ‚úÖ Download checkpoints regularly<br>
        ‚úÖ Check preview before processing<br>
        ‚úÖ Verify results after download
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center;">
        <strong style="color: #667eea;">Created by</strong><br>
        <strong>Abdelmoneim Moustafa</strong><br>
        <small>Data Intelligence Engineer</small>
    </div>
    """, unsafe_allow_html=True)

# File upload section
st.markdown("### üì§ Upload Your Data")
uploaded_file = st.file_uploader(
    "Choose Excel file containing Swagelok product URLs",
    type=["xlsx", "xls"],
    help="File must contain at least one column with URLs"
)

if uploaded_file:
    df = pd.read_excel(uploaded_file)
    
    # Auto-detect URL column
    url_column = None
    for col in df.columns:
        if df[col].astype(str).str.contains("http", na=False, case=False).any():
            url_column = col
            break
    
    if not url_column:
        st.error("‚ùå No URL column detected in file")
        st.stop()
    
    # Get unique URLs
    urls_series = df[url_column].dropna().astype(str)
    unique_urls = urls_series.drop_duplicates().tolist()
    
    st.success(f"‚úÖ File loaded successfully! Detected column: **{url_column}**")
    
    # Statistics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üìä Total URLs", len(urls_series))
    with col2:
        st.metric("‚≠ê Unique URLs", len(unique_urls))
    with col3:
        st.metric("üóëÔ∏è Duplicates Removed", len(urls_series) - len(unique_urls))
    
    # Warning for large files
    if len(unique_urls) > 500:
        st.markdown(f"""
        <div class="checkpoint-box">
            <strong>‚ö†Ô∏è Large File Detected</strong><br>
            You have {len(unique_urls)} URLs. Consider processing in batches of 500 for optimal performance.<br>
            Progress will be auto-saved every 50 URLs.
        </div>
        """, unsafe_allow_html=True)
    
    # Preview
    with st.expander("üëÅÔ∏è Preview Sample URLs"):
        st.dataframe(pd.DataFrame({url_column: unique_urls[:10]}), use_container_width=True)
    
    st.markdown("---")
    
    # Extract button
    if st.button("üöÄ Start Intelligent Extraction", type="primary", use_container_width=True):
        
        extractor = ProductionExtractor()
        results = []
        
        progress_bar = st.progress(0)
        status = st.empty()
        checkpoint_info = st.empty()
        results_placeholder = st.empty()
        
        start_time = time.time()
        checkpoint_count = 0
        
        # Process in batches
        batch_size = CHECKPOINT_INTERVAL
        total_batches = (len(unique_urls) + batch_size - 1) // batch_size
        
        for batch_idx in range(total_batches):
            batch_start = batch_idx * batch_size
            batch_end = min((batch_idx + 1) * batch_size, len(unique_urls))
            batch_urls = unique_urls[batch_start:batch_end]
            
            status.markdown(f"""
            <div class="progress-text">
                üì¶ Processing Batch {batch_idx + 1} of {total_batches}
            </div>
            """, unsafe_allow_html=True)
            
            # Process batch
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(extractor.extract, url) for url in batch_urls]
                
                for i, future in enumerate(as_completed(futures), 1):
                    try:
                        result = future.result(timeout=30)
                        if result is not None:
                            results.append(result)
                    except Exception:
                        pass
                    
                    # Update progress
                    total_processed = batch_start + i
                    progress = total_processed / len(unique_urls)
                    progress_bar.progress(progress)
                    
                    elapsed = time.time() - start_time
                    speed = total_processed / elapsed if elapsed > 0 else 0
                    
                    status.markdown(f"""
                    <div class="progress-text">
                        ‚ö° {total_processed}/{len(unique_urls)} | {speed:.1f} URLs/sec
                    </div>
                    """, unsafe_allow_html=True)
            
            # Checkpoint
            checkpoint_count += 1
            if len(results) > 0:
                checkpoint_df = pd.DataFrame(results)
                checkpoint_df = checkpoint_df.drop_duplicates(subset=['Part'], keep='first')
                checkpoint_df = checkpoint_df.drop_duplicates(subset=['URL'], keep='first')
                
                checkpoint_info.markdown(f"""
                <div class="checkpoint-box">
                    ‚úÖ <strong>Checkpoint {checkpoint_count}:</strong> {len(results)} results saved
                </div>
                """, unsafe_allow_html=True)
                
                # Checkpoint download
                buffer = BytesIO()
                with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
                    checkpoint_df.to_excel(writer, index=False, sheet_name="UNSPSC Results")
                
                results_placeholder.download_button(
                    label=f"üíæ Download Checkpoint {checkpoint_count} ({len(results)} results)",
                    data=buffer.getvalue(),
                    file_name=f"swagelok_checkpoint_{checkpoint_count}_{int(time.time())}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"checkpoint_{checkpoint_count}"
                )
        
        # Final processing
        total_time = int(time.time() - start_time)
        
        output_df = pd.DataFrame(results)
        output_df = output_df.drop_duplicates(subset=['Part'], keep='first')
        output_df = output_df.drop_duplicates(subset=['URL'], keep='first')
        
        parts_found = (output_df["Part"] != "Not Found").sum()
        unspsc_found = (output_df["UNSPSC Code"] != "Not Found").sum()
        
        # Success message
        st.markdown(f"""
        <div class="success-box">
            <h2>‚úÖ Extraction Complete!</h2>
            <p style="font-size: 1.2rem; margin: 1rem 0;">
                <strong>{len(output_df)}</strong> unique products processed in <strong>{total_time}</strong> seconds
            </p>
            <p style="font-size: 1.1rem;">
                Parts Found: <strong>{parts_found}</strong> ‚Ä¢ UNSPSC Found: <strong>{unspsc_found}</strong>
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Final metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("‚úÖ Parts Found", parts_found, f"{parts_found/len(output_df)*100:.1f}%")
        with col2:
            st.metric("‚úÖ UNSPSC Found", unspsc_found, f"{unspsc_found/len(output_df)*100:.1f}%")
        with col3:
            st.metric("‚è±Ô∏è Total Time", f"{total_time}s")
        with col4:
            st.metric("üöÄ Speed", f"{len(output_df)/total_time:.1f}/s")
        
        # Final download
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
            output_df.to_excel(writer, index=False, sheet_name="UNSPSC Results")
        
        st.download_button(
            label="üì• Download Complete Results (Excel)",
            data=buffer.getvalue(),
            file_name=f"swagelok_complete_{int(time.time())}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        # Results preview
        st.markdown("### üìã Results Preview")
        st.dataframe(output_df, use_container_width=True, height=400)

# Footer
st.markdown("---")
st.markdown("""
<div class="footer">
    <p class="footer-name">üé® Designed & Developed by Abdelmoneim Moustafa</p>
    <p>Data Intelligence Engineer ‚Ä¢ Automation Specialist ‚Ä¢ Procurement Systems Expert</p>
    <p style="margin-top: 1rem; font-size: 0.9rem; color: #999;">
        ¬© 2025 Swagelok UNSPSC Intelligence Platform ‚Ä¢ Production Version
    </p>
</div>
""", unsafe_allow_html=True)
