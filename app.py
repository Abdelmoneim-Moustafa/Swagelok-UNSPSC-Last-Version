"""
üîç Swagelok UNSPSC Intelligence Platform

EXTRACTION RULES:
1. Part #: Extract from "Part #:" label on page
2. UNSPSC: Get LATEST version from specifications table
3. Validation: Part from page MUST match part in URL
4. Uniqueness: Each URL processed only once

Created by: Abdelmoneim Moustafa
Data Intelligence Engineer
"""

import re
import time
import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, Optional, Set

# ==================== CONFIG ====================
MAX_WORKERS = 10
COMPANY_NAME = "Swagelok"

# ==================== PAGE CONFIG ====================
st.set_page_config(
    page_title="Swagelok UNSPSC Platform",
    page_icon="üîç",
    layout="wide"
)

# ==================== CSS ====================
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea, #764ba2);
        padding: 3rem 2rem;
        border-radius: 15px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .success-box {
        background: linear-gradient(135deg, #11998e, #38ef7d);
        padding: 2rem;
        border-radius: 12px;
        color: white;
        text-align: center;
        margin: 1.5rem 0;
    }
    .info-box {
        background: #e3f2fd;
        border-left: 4px solid #2196f3;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# ==================== PERFECT EXTRACTOR ====================
class SwagelokPerfectExtractor:
    """
    Perfect extractor based on actual page structure
    - Extracts from "Part #:" label
    - Gets latest UNSPSC from specs table
    - Validates part matches URL
    - Ensures uniqueness
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0"
        })
        self.processed_urls: Set[str] = set()
        self.extracted_parts: Set[str] = set()
    
    def extract(self, url: str) -> Optional[Dict]:
        """
        Extract data with strict validation
        Returns None if:
        - URL already processed
        - Part already extracted
        - Part doesn't match URL
        """
        
        # Normalize URL
        url = str(url).strip()
        
        # Check if already processed
        if url in self.processed_urls:
            return None
        
        # Initialize result
        result = {
            "Part": "Not Found",
            "Company": COMPANY_NAME,
            "URL": url,
            "UNSPSC Feature (Latest)": "Not Found",
            "UNSPSC Code": "Not Found"
        }
        
        # Validate URL
        if not self._is_valid_url(url):
            return None
        
        try:
            # Fetch page
            response = self.session.get(url, timeout=30)
            
            if response.status_code != 200:
                self.processed_urls.add(url)
                return result
            
            soup = BeautifulSoup(response.text, "html.parser")
            html_text = response.text
            
            # STEP 1: Extract part number from page
            part_from_page = self._extract_part_from_page(soup, html_text)
            
            # STEP 2: Extract part from URL for validation
            part_from_url = self._extract_part_from_url(url)
            
            # STEP 3: Validate - parts must match!
            if part_from_page and part_from_url:
                # Normalize for comparison
                page_normalized = self._normalize_part(part_from_page)
                url_normalized = self._normalize_part(part_from_url)
                
                # Check if they match
                if page_normalized == url_normalized:
                    # Check uniqueness
                    if part_from_page in self.extracted_parts:
                        self.processed_urls.add(url)
                        return None  # Skip duplicate
                    
                    result["Part"] = part_from_page
                    self.extracted_parts.add(part_from_page)
                else:
                    # Parts don't match - use URL part as fallback
                    if part_from_url not in self.extracted_parts:
                        result["Part"] = part_from_url
                        self.extracted_parts.add(part_from_url)
            
            elif part_from_page:
                # Only page part found
                if part_from_page not in self.extracted_parts:
                    result["Part"] = part_from_page
                    self.extracted_parts.add(part_from_page)
            
            elif part_from_url:
                # Only URL part found
                if part_from_url not in self.extracted_parts:
                    result["Part"] = part_from_url
                    self.extracted_parts.add(part_from_url)
            
            # STEP 4: Extract LATEST UNSPSC
            feature, code = self._extract_latest_unspsc(soup, html_text)
            if feature and code:
                result["UNSPSC Feature (Latest)"] = feature
                result["UNSPSC Code"] = code
            
            # Mark as processed
            self.processed_urls.add(url)
            
            return result
            
        except Exception as e:
            print(f"Error: {url} - {str(e)}")
            self.processed_urls.add(url)
            return result
    
    def _is_valid_url(self, url: str) -> bool:
        """Validate URL format"""
        if not isinstance(url, str):
            return False
        if not url.startswith("http"):
            return False
        if "swagelok.com" not in url.lower():
            return False
        return True
    
    def _extract_part_from_page(self, soup: BeautifulSoup, html_text: str) -> Optional[str]:
        """
        Extract part from "Part #:" label
        Based on screenshot: "Part #: CWS-C.040-.405-P"
        """
        
        # Method 1: Look for "Part #:" pattern in HTML
        patterns = [
            r'Part\s*#\s*:\s*([A-Z0-9][A-Z0-9.\-_/]+)',
            r'Part\s+#\s*:\s*([A-Z0-9][A-Z0-9.\-_/]+)',
            r'Part\s+Number\s*:\s*([A-Z0-9][A-Z0-9.\-_/]+)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_text, re.IGNORECASE)
            for match in matches:
                if self._is_valid_part(match):
                    return match.strip()
        
        # Method 2: Look in specific HTML elements
        # Check for breadcrumb or highlighted text
        for tag in soup.find_all(['span', 'strong', 'div', 'h1', 'h2']):
            text = tag.get_text(strip=True)
            if 'Part #' in text or 'Part#' in text:
                # Extract part after "Part #:"
                match = re.search(r'Part\s*#?\s*:\s*([A-Z0-9.\-_/]+)', text, re.IGNORECASE)
                if match and self._is_valid_part(match.group(1)):
                    return match.group(1).strip()
        
        return None
    
    def _extract_part_from_url(self, url: str) -> Optional[str]:
        """
        Extract part from URL parameter
        Examples:
        - ?part=CWS-C.040-.405-P
        - /p/CWS-C.040-.405-P
        """
        
        # URL patterns
        patterns = [
            r'[?&]part=([A-Z0-9.\-_/%]+)',
            r'/p/([A-Z0-9.\-_/%]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url, re.IGNORECASE)
            if match:
                part = match.group(1)
                # URL decode
                part = part.replace('%2F', '/').replace('%252F', '/')
                part = part.replace('%20', ' ').replace('%2E', '.')
                part = part.strip()
                if self._is_valid_part(part):
                    return part
        
        return None
    
    def _normalize_part(self, part: str) -> str:
        """
        Normalize part number for comparison
        Examples:
        - "CWS-C.040-.405-P" ‚Üí "cwsc040405p"
        - "SS-4C-1/3" ‚Üí "ss4c13"
        """
        if not part:
            return ""
        # Remove dots, dashes, slashes, spaces - keep only alphanumeric
        normalized = re.sub(r'[.\-/\s]', '', part)
        return normalized.lower()
    
    def _is_valid_part(self, part: str) -> bool:
        """Validate part number"""
        if not isinstance(part, str):
            return False
        
        part = part.strip()
        
        if not (2 <= len(part) <= 100):
            return False
        
        # Must have letters AND numbers
        has_letter = any(c.isalpha() for c in part)
        has_number = any(c.isdigit() for c in part)
        
        if not (has_letter and has_number):
            return False
        
        # Exclude false positives
        exclude = ['charset', 'utf', 'html', 'text', 'http', 'www', 'content-type']
        part_lower = part.lower()
        
        return not any(ex in part_lower for ex in exclude)
    
    def _extract_latest_unspsc(self, soup: BeautifulSoup, html_text: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Extract LATEST UNSPSC from specifications table
        Based on screenshot:
        UNSPSC (17.1001) | 39120000 ‚Üê This is the latest
        """
        
        versions = []
        
        # Parse specifications table
        for row in soup.find_all('tr'):
            cells = row.find_all('td')
            
            if len(cells) >= 2:
                attribute = cells[0].get_text(strip=True)
                value = cells[1].get_text(strip=True)
                
                # Check if this is UNSPSC row
                # Pattern: UNSPSC (17.1001)
                version_match = re.search(r'UNSPSC\s*\(([\d.]+)\)', attribute, re.IGNORECASE)
                
                # Validate code (6-8 digits)
                if version_match and re.match(r'^\d{6,8}$', value):
                    version_str = version_match.group(1)
                    version_tuple = self._parse_version(version_str)
                    
                    versions.append({
                        'version': version_tuple,
                        'feature': attribute,
                        'code': value
                    })
        
        # Regex fallback
        if not versions:
            pattern = r'UNSPSC\s*\(([\d.]+)\)\s*[|:]\s*(\d{6,8})'
            matches = re.findall(pattern, html_text, re.IGNORECASE)
            
            for version_str, code in matches:
                versions.append({
                    'version': self._parse_version(version_str),
                    'feature': f"UNSPSC ({version_str})",
                    'code': code
                })
        
        if not versions:
            return None, None
        
        # Sort by version - HIGHEST = LATEST
        versions.sort(key=lambda x: x['version'], reverse=True)
        
        latest = versions[0]
        return latest['feature'], latest['code']
    
    def _parse_version(self, version_str: str) -> Tuple[int, ...]:
        """
        Parse version for comparison
        17.1001 ‚Üí (17, 1001)
        24.0701 ‚Üí (24, 701)
        """
        try:
            parts = version_str.split('.')
            return tuple(int(p) for p in parts)
        except:
            return (0,)

# ==================== STREAMLIT UI ====================

st.markdown("""
<div class="main-header">
    <h1>üîç Swagelok UNSPSC Intelligence Platform</h1>
    <h3>Perfect Extraction ‚Ä¢ Validated Data ‚Ä¢ Zero Duplicates</h3>
    <p>Based on actual page structure analysis</p>
</div>
""", unsafe_allow_html=True)

# Info box
st.markdown("""
<div class="info-box">
    <strong>‚úÖ EXTRACTION RULES:</strong><br>
    1. <strong>Validation:</strong> Part must match URL parameter<br>
    2. <strong>UNSPSC:</strong> Latest version from specifications table<br>
    3. <strong>Uniqueness:</strong> Each URL and part processed only once<br>
    4. <strong>Quality:</strong> Invalid data filtered automatically
</div>
""", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.markdown("### üìñ How It Works")
    st.info("""
    **Extraction Process:**
    1. Visit product page
    2. Find "Part #:" label
    3. Extract complete part number
    4. Validate against URL
    5. Get latest UNSPSC version
    6. Check uniqueness
    7. Return clean data
    """)
    
    st.markdown("### üéØ Quality Checks")
    st.success("""
    ‚úÖ Part matches URL
    ‚úÖ No duplicates
    ‚úÖ Valid format
    ‚úÖ Latest UNSPSC
    ‚úÖ Complete data
    """)
    
    st.markdown("---")
    st.caption("üé® Abdelmoneim Moustafa\nData Intelligence Engineer")

# File upload
uploaded_file = st.file_uploader(
    "üì§ Upload Excel file with URL column",
    type=["xlsx", "xls"],
    help="File must contain Swagelok product URLs"
)

if uploaded_file:
    # Read file
    df = pd.read_excel(uploaded_file)
    
    # Auto-detect URL column
    url_column = None
    for col in df.columns:
        if df[col].astype(str).str.contains("http", na=False, case=False).any():
            url_column = col
            break
    
    if not url_column:
        st.error("‚ùå No URL column found")
        st.stop()
    
    # Get unique URLs (preserve order)
    urls_series = df[url_column].dropna().astype(str)
    unique_urls = urls_series.drop_duplicates().tolist()
    
    st.success(f"‚úÖ File loaded! Column: **{url_column}**")
    
    # Stats
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üìä Total URLs", len(urls_series))
    with col2:
        st.metric("‚≠ê Unique URLs", len(unique_urls))
    with col3:
        st.metric("üóëÔ∏è Input Duplicates", len(urls_series) - len(unique_urls))
    
    # Preview
    with st.expander("üëÅÔ∏è Preview URLs (first 10)"):
        st.dataframe(pd.DataFrame({url_column: unique_urls[:10]}), use_container_width=True)
    
    st.markdown("---")
    
    # Extract button
    if st.button("üöÄ Start Perfect Extraction", type="primary", use_container_width=True):
        
        extractor = SwagelokPerfectExtractor()
        results = []
        
        progress_bar = st.progress(0)
        status = st.empty()
        
        start_time = time.time()
        
        # Process URLs
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(extractor.extract, url) for url in unique_urls]
            
            for i, future in enumerate(as_completed(futures), 1):
                result = future.result()
                
                # Only add valid results (not None)
                if result is not None:
                    results.append(result)
                
                # Update progress
                progress = i / len(unique_urls)
                progress_bar.progress(progress)
                
                elapsed = time.time() - start_time
                speed = i / elapsed if elapsed > 0 else 0
                remaining = int((len(unique_urls) - i) / speed) if speed > 0 else 0
                
                status.text(f"‚ö° {i}/{len(unique_urls)} | Speed: {speed:.1f}/s | Remaining: {remaining}s")
        
        total_time = int(time.time() - start_time)
        
        # Create output
        output_df = pd.DataFrame(results)
        
        # Remove any remaining duplicates (shouldn't happen, but safety check)
        output_df = output_df.drop_duplicates(subset=['Part'], keep='first')
        output_df = output_df.drop_duplicates(subset=['URL'], keep='first')
        
        # Stats
        parts_found = (output_df["Part"] != "Not Found").sum()
        unspsc_found = (output_df["UNSPSC Code"] != "Not Found").sum()
        
        # Success message
        st.markdown(f"""
        <div class="success-box">
            <h2>‚úÖ Perfect Extraction Complete!</h2>
            <p>{len(results)} unique products processed in {total_time} seconds</p>
            <p>Parts: {parts_found} found ‚Ä¢ UNSPSC: {unspsc_found} found</p>
            <p>All data validated and deduplicated!</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("‚úÖ Parts Found", parts_found, f"{parts_found/len(results)*100:.1f}%")
        with col2:
            st.metric("‚úÖ UNSPSC Found", unspsc_found, f"{unspsc_found/len(results)*100:.1f}%")
        with col3:
            st.metric("‚è±Ô∏è Time", f"{total_time}s")
        with col4:
            st.metric("üöÄ Speed", f"{len(results)/total_time:.1f}/s")
        
        # Download
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine="openpyxl") as writer:
            output_df.to_excel(writer, index=False, sheet_name="Results")
        
        st.download_button(
            label="üì• Download Perfect Results (Excel)",
            data=buffer.getvalue(),
            file_name=f"swagelok_perfect_{int(time.time())}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            use_container_width=True
        )
        
        # Results preview
        st.markdown("### üìã Results Preview")
        st.dataframe(output_df, use_container_width=True, height=400)
        
        # Sample details
        with st.expander("üîç Sample Extraction Details (First 5)"):
            for i, row in output_df.head(5).iterrows():
                st.markdown(f"""
                **Product {i+1}:**
                - **Part:** `{row['Part']}`
                - **UNSPSC:** {row['UNSPSC Feature (Latest)']} = `{row['UNSPSC Code']}`
                - **URL:** {row['URL'][:60]}...
                ---
                """)

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; padding: 1rem;">
    üé® <strong>Designed & Developed by Abdelmoneim Moustafa</strong><br>
    Data Intelligence Engineer ‚Ä¢ Procurement Systems Expert<br>
    <small>¬© 2025 Swagelok UNSPSC Platform - Perfect Version</small>
</div>
""", unsafe_allow_html=True)
